{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.68, 82, 74)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "neu = '0'\n",
    "pos = '0'\n",
    "neg = '1'\n",
    "title='aif'\n",
    "\n",
    "f = open('/home/ydw/capston/python/data/sum(fromPH.D+twitter)/text.txt', 'r', encoding='utf-8')\n",
    "text = f.read().splitlines()\n",
    "f.close()    \n",
    "f = open('/home/ydw/capston/python/data/sum(fromPH.D+twitter)/result.txt', 'r', encoding='utf-8')\n",
    "r_y = f.read().splitlines()\n",
    "f.close()\n",
    "\n",
    "y = []\n",
    "for i in r_y:\n",
    "    if(i == '2'):\n",
    "        y.append(neu)\n",
    "    elif(i == '0'):\n",
    "        y.append(pos)\n",
    "    else:\n",
    "        y.append(neg)\n",
    "        \n",
    "f = open('/home/ydw/capston/python/data/twitter/positive.txt', 'r', encoding='utf-8')\n",
    "p_text = f.read().splitlines()\n",
    "f.close()    \n",
    "\n",
    "f = open('/home/ydw/capston/python/data/twitter/negative.txt', 'r', encoding='utf-8')\n",
    "n_text = f.read().splitlines()\n",
    "f.close()    \n",
    "\n",
    "p_text = p_text[35000:35000+3000]\n",
    "n_text = n_text[35000:35000+3000]\n",
    "\n",
    "p_text = [' '.join(re.sub(\"(RT )|(@\\S+)|(\\w+:\\/\\/\\S+)|\", \"\", doc).split()) for doc in p_text]\n",
    "n_text = [' '.join(re.sub(\"(RT )|(@\\S+)|(\\w+:\\/\\/\\S+)|\", \"\", doc).split()) for doc in n_text]\n",
    "\n",
    "for i in p_text:\n",
    "    text.append(i)\n",
    "    y.append(pos)\n",
    "    \n",
    "for i in n_text:\n",
    "    text.append(i)\n",
    "    y.append(neg)\n",
    "\n",
    "#One-hot encoding Model\n",
    "text_train, text_test, y_train, y_test = train_test_split(text, y, test_size=0.2, random_state=1)\n",
    "pipe_b = make_pipeline(CountVectorizer(ngram_range=(1,3), min_df=0), LogisticRegression())\n",
    "pipe_b.fit(text_train, y_train)\n",
    "\n",
    "f = open('/home/ydw/capston/python/data/application/reply_{}_100.txt'.format(title), 'r', encoding='utf-8')\n",
    "text = f.read().splitlines()\n",
    "f.close()    \n",
    "\n",
    "f = open('/home/ydw/capston/python/data/application/{}.txt'.format(title), 'r', encoding='utf-8')\n",
    "r_y = f.read().splitlines()\n",
    "f.close()    \n",
    "\n",
    "y = []\n",
    "for i in r_y:\n",
    "    if(i == 'l'):\n",
    "        y.append(neu)\n",
    "    elif(i == 'p'):\n",
    "        y.append(pos)\n",
    "    elif(i == 'n'):\n",
    "        y.append(neg)\n",
    "    else:\n",
    "        y.append('NA')\n",
    "        \n",
    "pre_y = []\n",
    "for doc in text:\n",
    "    rdoc = [doc]\n",
    "    pre_y.append(pipe_b.predict(rdoc)[0])\n",
    "    \n",
    "count = 0\n",
    "for i in range(len(y)):\n",
    "    if(y[i] == pre_y[i]):\n",
    "        count = count+1\n",
    "        \n",
    "a = pre_y\n",
    "count/len(y), pre_y.count('0'), y.count('0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.83, 91, 86)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import fasttext\n",
    "\n",
    "neu = '0'\n",
    "pos = '1'\n",
    "neg = '0'\n",
    "title='aif'\n",
    "\n",
    "f = open('/home/ydw/capston/python/data/expert/neutral.txt', 'r', encoding='utf-8')\n",
    "text = f.read().splitlines()\n",
    "f.close()    \n",
    "\n",
    "y = []\n",
    "for i in range(len(text)):\n",
    "    y.append(neu)\n",
    "\n",
    "f = open('/home/ydw/capston/python/data/twitter/positive.txt', 'r', encoding='utf-8')\n",
    "p_text = f.read().splitlines()\n",
    "f.close()    \n",
    "\n",
    "f = open('/home/ydw/capston/python/data/twitter/negative.txt', 'r', encoding='utf-8')\n",
    "n_text = f.read().splitlines()\n",
    "f.close()    \n",
    "\n",
    "p_text = p_text[35000:35000+3000]\n",
    "n_text = n_text[35000:35000+3000]\n",
    "\n",
    "p_text = [' '.join(re.sub(\"(RT )|(@\\S+)|(\\w+:\\/\\/\\S+)|\", \"\", doc).split()) for doc in p_text]\n",
    "n_text = [' '.join(re.sub(\"(RT )|(@\\S+)|(\\w+:\\/\\/\\S+)|\", \"\", doc).split()) for doc in n_text]\n",
    "\n",
    "for i in p_text:\n",
    "    text.append(i)\n",
    "    y.append(pos)\n",
    "    \n",
    "for i in n_text:\n",
    "    text.append(i)\n",
    "    y.append(neg)\n",
    "\n",
    "#fasttext Model\n",
    "for i in range(len(text)):\n",
    "    text[i] = '__label__'+y[i]+' '+text[i]+'\\n'\n",
    "    \n",
    "fasttext_train = open(\"/home/ydw/capston/python/data/sum(fromPH.D+twitter)/fasttext_data/fasttext_train.txt\", \"w\", encoding='utf-8')\n",
    "fasttext_test = open(\"/home/ydw/capston/python/data/sum(fromPH.D+twitter)/fasttext_data/fasttext_test.txt\", \"w\", encoding='utf-8')\n",
    "for i in text:\n",
    "    if(np.random.uniform() < 0.2):\n",
    "        fasttext_test.write(i)\n",
    "    else:\n",
    "        fasttext_train.write(i)\n",
    "        \n",
    "classifier = fasttext.supervised('/home/ydw/capston/python/data/sum(fromPH.D+twitter)/fasttext_data/fasttext_train.txt',\n",
    "                                 '/home/ydw/capston/python/data/sum(fromPH.D+twitter)/fasttext_data/classifier',\n",
    "                                 dim=20, loss='softmax', epoch=10, label_prefix='__label__')\n",
    "\n",
    "f = open('/home/ydw/capston/python/data/application/reply_{}_100.txt'.format(title), 'r', encoding='utf-8')\n",
    "text = f.read().splitlines()\n",
    "f.close()    \n",
    "\n",
    "f = open('/home/ydw/capston/python/data/application/{}.txt'.format(title), 'r', encoding='utf-8')\n",
    "r_y = f.read().splitlines()\n",
    "f.close()    \n",
    "\n",
    "y = []\n",
    "for i in r_y:\n",
    "    if(i == 'l'):\n",
    "        y.append(neu)\n",
    "    elif(i == 'p'):\n",
    "        y.append(pos)\n",
    "    elif(i == 'n'):\n",
    "        y.append(neg)\n",
    "    else:\n",
    "        y.append('NA')\n",
    "        \n",
    "pre_y = []\n",
    "for doc in text:\n",
    "    rdoc = [doc]\n",
    "    pre_y.append(classifier.predict(rdoc)[0][0])\n",
    "\n",
    "count = 0\n",
    "for i in range(len(y)):\n",
    "    if(y[i] == pre_y[i]):\n",
    "        count = count+1\n",
    "        \n",
    "b = pre_y\n",
    "count/len(y), pre_y.count('0'), y.count('0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = []\n",
    "re_text = []\n",
    "for i in range(len(a)):\n",
    "    if(a[i]=='1' and b[i]=='1'):\n",
    "        index.append(i)\n",
    "        re_text.append(text[i])\n",
    "        a[i] = '3'\n",
    "    elif(a[i]=='0' and b[i]=='1'):\n",
    "        a[i] = '0'\n",
    "    elif(a[i]=='0' and b[i]=='0'):\n",
    "        a[i] = '2'        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=0,\n",
       "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
       "  ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('/home/ydw/capston/python/data/twitter/positive.txt', 'r', encoding='utf-8')\n",
    "p_text = f.read().splitlines()\n",
    "f.close()    \n",
    "\n",
    "f = open('/home/ydw/capston/python/data/twitter/negative.txt', 'r', encoding='utf-8')\n",
    "n_text = f.read().splitlines()\n",
    "f.close()    \n",
    "\n",
    "p_text = p_text[:20000]\n",
    "n_text = n_text[:20000]\n",
    "\n",
    "p_text = [' '.join(re.sub(\"(RT )|(@\\S+)|(\\w+:\\/\\/\\S+)|\", \"\", doc).split()) for doc in p_text]\n",
    "n_text = [' '.join(re.sub(\"(RT )|(@\\S+)|(\\w+:\\/\\/\\S+)|\", \"\", doc).split()) for doc in n_text]\n",
    "\n",
    "text = []\n",
    "y = []\n",
    "\n",
    "for i in p_text:\n",
    "    text.append(i)\n",
    "    y.append('0')\n",
    "    \n",
    "for i in n_text:\n",
    "    text.append(i)\n",
    "    y.append('1')\n",
    "\n",
    "text_train, text_test, y_train, y_test = train_test_split(text, y, test_size=0.2, random_state=1)\n",
    "pipe_n = make_pipeline(CountVectorizer(ngram_range=(1,3), min_df=0), LogisticRegression())\n",
    "pipe_n.fit(text_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_re_y = []\n",
    "for doc in re_text:\n",
    "    rdoc = [doc]\n",
    "    pre_re_y.append(pipe_n.predict(rdoc)[0])\n",
    "    \n",
    "k = 0\n",
    "for i in index:\n",
    "    a[i] = pre_re_y[k]\n",
    "    k=k+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.53\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12, 9, 24, 18, 62, 73)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('/home/ydw/capston/python/data/application/{}.txt'.format(title), 'r', encoding='utf-8')\n",
    "r_y = f.read().splitlines()\n",
    "f.close()    \n",
    "\n",
    "y = []\n",
    "for i in r_y:\n",
    "    if(i == 'l'):\n",
    "        y.append('2')\n",
    "    elif(i == 'p'):\n",
    "        y.append('0')\n",
    "    elif(i == 'n'):\n",
    "        y.append('1')\n",
    "    else:\n",
    "        y.append('NA')\n",
    "        \n",
    "count = 0\n",
    "for i in range(len(y)):\n",
    "    if(y[i] == a[i]):\n",
    "        count = count+1\n",
    "print(count/len(y))\n",
    "\n",
    "y.count('0'), a.count('0'),y.count('1'), a.count('1'),y.count('2'), a.count('2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
